{"cells":[{"cell_type":"code","source":["def list_fabric_workspaces(token):\n","    \"\"\"\n","    Calls Fabric REST: GET https://api.fabric.microsoft.com/v1/workspaces\n","    Handles simple pagination using 'continuationToken' if returned.\n","    Returns: list of workspace dicts.\n","    \"\"\"\n","    # token = acquire_token_device_code(FABRIC_SCOPE)\n","    headers = {\"Authorization\": f\"Bearer {token}\", \"Accept\": \"application/json\"}\n","\n","    base_url = \"https://api.fabric.microsoft.com/v1/workspaces\"\n","    workspaces = []\n","\n","\n","    # Initial page\n","    url = base_url\n","    while True:\n","        resp = requests.get(url, headers=headers, timeout=60)\n","        if resp.status_code == 429:\n","            # Optional: respect Retry-After for throttling\n","            retry_after = int(resp.headers.get(\"Retry-After\", \"2\"))\n","            import time; time.sleep(retry_after)\n","            continue\n","        resp.raise_for_status()\n","        payload = resp.json() if resp.headers.get(\"Content-Type\",\"\").startswith(\"application/json\") else {}\n","        page = payload.get(\"value\", [])\n","        workspaces.extend(page)\n","\n","        # Handle server-side pagination if continuation token is present\n","        cont = payload.get(\"continuationToken\") or payload.get(\"@odata.nextLink\")  # depending on API shape\n","        if cont:\n","            # If continuation is a token, append as query param; if it's a URL, use it directly\n","            if isinstance(cont, str) and cont.startswith(\"http\"):\n","                url = cont\n","            else:\n","                # Some Fabric endpoints return a 'continuationToken' that must be passed as a query param\n","                url = f\"{base_url}?continuationToken={cont}\"\n","        else:\n","            break\n","\n","    return workspaces\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"4f808cfa-6cec-4877-96fb-624ee00d8d43","normalized_state":"finished","queued_time":"2026-02-22T17:58:10.8168307Z","session_start_time":"2026-02-22T17:58:10.8178278Z","execution_start_time":"2026-02-22T17:58:21.5889014Z","execution_finish_time":"2026-02-22T17:58:21.8395755Z","parent_msg_id":"3c096926-04f5-48fd-a115-e5cc4bd5aa9e"},"text/plain":"StatementMeta(, 4f808cfa-6cec-4877-96fb-624ee00d8d43, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f9d1606f-a278-4778-845b-0aff86f0fc70"},{"cell_type":"code","source":["def start_scan(workspace_ids, lineage=True, datasourceDetails=True,\n","               datasetSchema=False, datasetExpressions=False, getArtifactUsers=False):\n","    \"\"\"\n","    workspace_ids: list of workspace GUID strings (1..100 per call)\n","    returns scan request object: {id, createdDateTime, status}\n","    \"\"\"\n","    if not (1 <= len(workspace_ids) <= 100):\n","        raise ValueError(\"workspace_ids must contain 1..100 IDs per scan request\")\n","\n","    params = {\n","        \"lineage\": str(lineage).lower(),\n","        \"datasourceDetails\": str(datasourceDetails).lower(),\n","        \"datasetSchema\": str(datasetSchema).lower(),\n","        \"datasetExpressions\": str(datasetExpressions).lower(),\n","        \"getArtifactUsers\": str(getArtifactUsers).lower(),\n","    }\n","\n","    url = f\"{BASE}/admin/workspaces/getInfo\"\n","    resp = requests.post(url, headers=headers, params=params,\n","                         json={\"workspaces\": workspace_ids})\n","    if resp.status_code != 202:\n","        raise RuntimeError(f\"getInfo failed {resp.status_code}: {resp.text}\")\n","    return resp.json()\n","\n","# Example: single workspace\n","#print(workspace_ids)\n","#workspace_id = \"59453e5f-1b41-4ca9-a493-c72eda9e5a50\"\n","#scan_req = start_scan([workspace_ids], lineage=True, datasourceDetails=True)\n","#scan_id = scan_req[\"id\"]\n","#scan_req\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"4f808cfa-6cec-4877-96fb-624ee00d8d43","normalized_state":"finished","queued_time":"2026-02-22T17:58:10.8861138Z","session_start_time":null,"execution_start_time":"2026-02-22T17:58:21.8417888Z","execution_finish_time":"2026-02-22T17:58:22.082281Z","parent_msg_id":"721eb44e-3eb6-475d-85aa-3e7b4203a24d"},"text/plain":"StatementMeta(, 4f808cfa-6cec-4877-96fb-624ee00d8d43, 4, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7987c232-33f0-42ce-b842-cf2a753624cf"},{"cell_type":"code","source":["def get_scan_status(scan_id=\"\"):\n","    url = f\"{BASE}/admin/workspaces/scanStatus/{scan_id}\"\n","    resp = requests.get(url, headers=headers)\n","    if resp.status_code != 200:\n","        raise RuntimeError(f\"scanStatus failed {resp.status_code}: {resp.text}\")\n","    return resp.json()\n","\n","# poll\n","#while True:\n","#    st = get_scan_status(scan_id)\n","#    status = st.get(\"status\")\n","#    print(\"scanStatus =\", status)\n","#    if status in (\"Succeeded\", \"Failed\"):\n","#        break\n","#    time.sleep(3)\n","\n","#if status != \"Succeeded\":\n","#    raise RuntimeError(f\"Scan failed: {st}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"4f808cfa-6cec-4877-96fb-624ee00d8d43","normalized_state":"finished","queued_time":"2026-02-22T17:58:10.9756683Z","session_start_time":null,"execution_start_time":"2026-02-22T17:58:22.0842043Z","execution_finish_time":"2026-02-22T17:58:22.3165175Z","parent_msg_id":"1d99f6c4-06b9-45c8-b945-31957e15877a"},"text/plain":"StatementMeta(, 4f808cfa-6cec-4877-96fb-624ee00d8d43, 5, Finished, Available, Finished)"},"metadata":{}}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"08e76a91-47ce-4975-bf52-e0d5636a7275"},{"cell_type":"code","source":["def get_scan_result(scan_id):\n","    url = f\"{BASE}/admin/workspaces/scanResult/{scan_id}\"\n","    resp = requests.get(url, headers=headers)\n","    if resp.status_code != 200:\n","        raise RuntimeError(f\"scanResult failed {resp.status_code}: {resp.text}\")\n","    return resp.json()\n","\n","#scan_result = get_scan_result(scan_id)\n","#scan_result.keys()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"4f808cfa-6cec-4877-96fb-624ee00d8d43","normalized_state":"finished","queued_time":"2026-02-22T17:58:11.1678226Z","session_start_time":null,"execution_start_time":"2026-02-22T17:58:22.3185559Z","execution_finish_time":"2026-02-22T17:58:22.5858122Z","parent_msg_id":"b70db700-66ae-470a-bbad-9dd8d3710692"},"text/plain":"StatementMeta(, 4f808cfa-6cec-4877-96fb-624ee00d8d43, 6, Finished, Available, Finished)"},"metadata":{}}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d3b71513-cfe7-4722-ba1c-e241d6301c5c"},{"cell_type":"code","source":["def save_scan_result_as_delta_table_dt(\n","    scan_id: str,\n","    scan_result: dict,\n","    run_dt,  # <-- datetime you pass in (same for workspaces + scans)\n","    table_name: str = \"pbi_scan_results\"\n","):\n","    # 1) Persist raw JSON too (optional but handy for debugging/audit)\n","    from pyspark.sql import Row\n","    from pyspark.sql.types import StructType, StructField, StringType\n","    import json\n","\n","    # normalize to a stable string once (UTC recommended)\n","    run_dt_str = run_dt.isoformat()\n","\n","    # (optional) include run timestamp in file name for easy correlation\n","    raw_path = f\"Files/pbi_scans/raw/scanResult_{scan_id}_{run_dt_str}.json\"\n","\n","    mssparkutils.fs.mkdirs(\"Files/delta_tables/raw\")\n","    mssparkutils.fs.put(raw_path, json.dumps(scan_result, ensure_ascii=False), overwrite=True)  # [1](https://microsoft-my.sharepoint.com/personal/noelleli_microsoft_com/Documents/Recordings/[US]%20Fabric%20Data%20Factory%20Feature%20Bug%20Bash-20260219_170322-Meeting%20Recording.mp4?web=1)\n","\n","    # 2) Create a minimal “raw JSON” delta table (schema-stable)\n","    payload_str = json.dumps(scan_result, ensure_ascii=False)\n","\n","    df = spark.createDataFrame(\n","        [Row(scan_id=scan_id, run_dt=run_dt_str, payload_json=payload_str)],\n","        schema=StructType([\n","            StructField(\"scan_id\", StringType(), False),\n","            StructField(\"run_dt\", StringType(), False),     # <-- added\n","            StructField(\"payload_json\", StringType(), False),\n","        ])\n","    )\n","\n","    (df.write\n","       .format(\"delta\")\n","       .mode(\"append\")\n","       .saveAsTable(table_name))\n","\n","    return raw_path, table_name"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"4f808cfa-6cec-4877-96fb-624ee00d8d43","normalized_state":"finished","queued_time":"2026-02-22T17:58:11.2876949Z","session_start_time":null,"execution_start_time":"2026-02-22T17:58:22.5877023Z","execution_finish_time":"2026-02-22T17:58:22.8412983Z","parent_msg_id":"587d59de-c35b-4271-8544-b3e69167de9c"},"text/plain":"StatementMeta(, 4f808cfa-6cec-4877-96fb-624ee00d8d43, 7, Finished, Available, Finished)"},"metadata":{}}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a6d51577-5d9e-46ef-93f0-ead2e6d6d1dd"},{"cell_type":"code","source":["# [2026-02-19 08:17 CST]\n","import json\n","from datetime import datetime, timezone\n","\n","from pyspark.sql import Row\n","from pyspark.sql.functions import col, explode, lit, from_json\n","from pyspark.sql.types import (\n","    StructType, StructField, StringType, BooleanType, ArrayType, TimestampType\n",")\n","\n","def persist_scan_to_graph_tables(\n","    scan_id: str,\n","    scan_result: dict,\n","    run_dt: datetime | None = None,\n","    mode: str = \"overwrite\",\n","    raw_table: str = \"pbi_scan_results_raw\",\n","    node_workspaces: str = \"pbi_graph_workspaces\",\n","    node_artifacts: str = \"pbi_graph_artifacts\",\n","    node_datasets: str = \"pbi_graph_datasets\",\n","    node_capacities: str = \"pbi_graph_capacities\",\n","    edge_ws_contains: str = \"pbi_graph_edge_workspace_contains_artifact\",\n","    edge_report_uses_dataset: str = \"pbi_graph_edge_report_uses_dataset\",\n","    edge_depends_on: str = \"pbi_graph_edge_artifact_depends_on_artifact\",\n","    edge_ws_on_capacity: str = \"pbi_graph_edge_workspace_on_capacity\",\n","):\n","    \"\"\"\n","    Persist Power BI/Fabric Scanner API GetScanResult payload into Delta tables suitable for Fabric Graph modeling.\n","\n","    IMPORTANT:\n","    - Pin your non-schema 'lineageGraph' lakehouse as the notebook default so saveAsTable lands in the right place. [3](https://microsoft-my.sharepoint.com/personal/gracegong_microsoft_com/_layouts/15/Doc.aspx?sourcedoc=%7B5A2A58F0-C231-41C5-8CD7-91AFD736BA2F%7D&file=Fabric%20Data%20Pipelines%20in%20Power%20BI%20%E2%80%93%20Advanced%20Semantic%20Model%20Refresh%20Customer%20Webcasts%20Summary%20Notes.docx&action=default&mobileredirect=true&DefaultItemOpen=1)\n","    - Graph doesn't support schema-enabled lakehouses today; use a non-schema lakehouse for these tables. [1](https://learn.microsoft.com/en-us/fabric/graph/tutorial-load-data)[2](https://learn.microsoft.com/en-us/fabric/graph/limitations)\n","    \"\"\"\n","\n","    if run_dt is None:\n","        run_dt = datetime.now(timezone.utc)\n","\n","    payload_json = json.dumps(scan_result, ensure_ascii=False)\n","\n","    # 1) Raw payload table (audit/debug/replay)\n","    raw_schema = StructType([\n","        StructField(\"scan_id\", StringType(), False),\n","        StructField(\"run_dt\", TimestampType(), False),\n","        StructField(\"payload_json\", StringType(), False),\n","    ])\n","\n","    raw_df = spark.createDataFrame(\n","        [Row(scan_id=scan_id, run_dt=run_dt, payload_json=payload_json)],\n","        schema=raw_schema\n","    )\n","\n","    raw_df.write.format(\"delta\").mode(\"append\").saveAsTable(raw_table)\n","\n","    # 2) Parse only what we need from the payload (resilient, optional fields)\n","    relation_schema = StructType([\n","        StructField(\"dependentOnArtifactId\", StringType(), True),\n","        StructField(\"workspaceId\", StringType(), True),\n","        StructField(\"relationType\", StringType(), True),\n","        StructField(\"settingsList\", StringType(), True),\n","        StructField(\"usage\", StringType(), True),\n","    ])\n","\n","    artifact_schema = StructType([\n","        StructField(\"id\", StringType(), True),\n","        StructField(\"name\", StringType(), True),\n","        StructField(\"description\", StringType(), True),\n","        StructField(\"state\", StringType(), True),\n","        StructField(\"createdDate\", StringType(), True),\n","        StructField(\"lastUpdatedDate\", StringType(), True),\n","        StructField(\"modifiedDate\", StringType(), True),\n","        StructField(\"modifiedDateTime\", StringType(), True),\n","        StructField(\"createdDateTime\", StringType(), True),\n","        StructField(\"datasetId\", StringType(), True),\n","        StructField(\"reportType\", StringType(), True),\n","        StructField(\"format\", StringType(), True),\n","        StructField(\"relations\", ArrayType(relation_schema), True),\n","        StructField(\"extendedProperties\", StringType(), True),\n","    ])\n","\n","    workspace_schema = StructType([\n","        StructField(\"id\", StringType(), True),\n","        StructField(\"name\", StringType(), True),\n","        StructField(\"type\", StringType(), True),\n","        StructField(\"state\", StringType(), True),\n","        StructField(\"isOnDedicatedCapacity\", BooleanType(), True),\n","        StructField(\"capacityId\", StringType(), True),\n","        StructField(\"defaultDatasetStorageFormat\", StringType(), True),\n","\n","        # NOTE: these keys match your payload casing\n","        StructField(\"Lakehouse\", ArrayType(artifact_schema), True),\n","        StructField(\"DataAgent\", ArrayType(artifact_schema), True),\n","        StructField(\"DataPipeline\", ArrayType(artifact_schema), True),\n","        StructField(\"Reflex\", ArrayType(artifact_schema), True),\n","        StructField(\"Notebook\", ArrayType(artifact_schema), True),\n","\n","        # reports is lower-case in your payload\n","        StructField(\"reports\", ArrayType(artifact_schema), True),\n","    ])\n","\n","    scan_schema = StructType([\n","        StructField(\"workspaces\", ArrayType(workspace_schema), True)\n","    ])\n","\n","    parsed = spark.createDataFrame(\n","        [Row(payload_json=payload_json)]\n","    ).select(\n","        from_json(col(\"payload_json\"), scan_schema).alias(\"scan\")\n","    )\n","\n","    # 3) Node: Workspaces\n","    ws_df = (parsed\n","        .select(explode(col(\"scan.workspaces\")).alias(\"w\"))\n","        .select(\n","            lit(scan_id).alias(\"scan_id\"),\n","            lit(run_dt).alias(\"run_dt\"),\n","            col(\"w.id\").alias(\"workspace_id\"),\n","            col(\"w.name\").alias(\"workspace_name\"),\n","            col(\"w.state\").alias(\"workspace_state\"),\n","            col(\"w.capacityId\").alias(\"capacity_id\"),\n","            col(\"w.defaultDatasetStorageFormat\").alias(\"default_dataset_storage_format\"),\n","        )\n","        .where(col(\"workspace_id\").isNotNull())\n","    )\n","\n","    # 4) Node: Capacity (from workspace.capacityId)\n","    cap_df = (ws_df\n","        .select(\n","            lit(scan_id).alias(\"scan_id\"),\n","            lit(run_dt).alias(\"run_dt\"),\n","            col(\"capacity_id\").alias(\"capacity_id\"),\n","        )\n","        .where(col(\"capacity_id\").isNotNull())\n","        .dropDuplicates([\"capacity_id\"])\n","    )\n","\n","    # Helper to explode any artifact array\n","    def explode_artifacts(array_col_name: str, artifact_type: str):\n","        return (parsed\n","            .select(explode(col(\"scan.workspaces\")).alias(\"w\"))\n","            .select(\n","                col(\"w.id\").alias(\"workspace_id\"),\n","                explode(col(f\"w.{array_col_name}\")).alias(\"a\")\n","            )\n","            .select(\n","                lit(scan_id).alias(\"scan_id\"),\n","                lit(run_dt).alias(\"run_dt\"),\n","                lit(artifact_type).alias(\"artifact_type\"),\n","                col(\"workspace_id\"),\n","                col(\"a.id\").alias(\"artifact_id\"),\n","                col(\"a.name\").alias(\"artifact_name\"),\n","                col(\"a.state\").alias(\"artifact_state\"),\n","                col(\"a.reportType\").alias(\"report_type\"),\n","                col(\"a.datasetId\").alias(\"dataset_id\"),\n","                col(\"a.format\").alias(\"format\"),\n","            )\n","            .where(col(\"artifact_id\").isNotNull())\n","        )\n","\n","    # 5) Node: Artifacts (lakehouse, data agent, pipeline, reflex, notebook, report)\n","    artifacts = [\n","        explode_artifacts(\"Lakehouse\", \"Lakehouse\"),\n","        explode_artifacts(\"DataAgent\", \"DataAgent\"),\n","        explode_artifacts(\"DataPipeline\", \"DataPipeline\"),\n","        explode_artifacts(\"Reflex\", \"Reflex\"),\n","        explode_artifacts(\"Notebook\", \"Notebook\"),\n","        explode_artifacts(\"reports\", \"Report\"),\n","    ]\n","\n","    artifacts_df = artifacts[0]\n","    for d in artifacts[1:]:\n","        artifacts_df = artifacts_df.unionByName(d, allowMissingColumns=True)\n","\n","    # 6) Node: Datasets (distinct datasetIds from reports)\n","    datasets_df = (artifacts_df\n","        .where((col(\"artifact_type\") == \"Report\") & col(\"dataset_id\").isNotNull())\n","        .select(\n","            lit(scan_id).alias(\"scan_id\"),\n","            lit(run_dt).alias(\"run_dt\"),\n","            col(\"dataset_id\").alias(\"dataset_id\")\n","        )\n","        .dropDuplicates([\"dataset_id\"])\n","    )\n","\n","    # 7) Edge: Workspace CONTAINS Artifact\n","    edge_ws_contains_df = (artifacts_df\n","        .select(\n","            lit(scan_id).alias(\"scan_id\"),\n","            lit(run_dt).alias(\"run_dt\"),\n","            col(\"workspace_id\").alias(\"from_workspace_id\"),\n","            col(\"artifact_id\").alias(\"to_artifact_id\"),\n","            col(\"artifact_type\").alias(\"to_artifact_type\"),\n","        )\n","        .where(col(\"from_workspace_id\").isNotNull() & col(\"to_artifact_id\").isNotNull())\n","    )\n","\n","    # 8) Edge: Report USES Dataset\n","    edge_report_uses_dataset_df = (artifacts_df\n","        .where((col(\"artifact_type\") == \"Report\") & col(\"dataset_id\").isNotNull())\n","        .select(\n","            lit(scan_id).alias(\"scan_id\"),\n","            lit(run_dt).alias(\"run_dt\"),\n","            col(\"artifact_id\").alias(\"from_report_id\"),\n","            col(\"dataset_id\").alias(\"to_dataset_id\"),\n","        )\n","    )\n","\n","    # 9) Edge: Artifact DEPENDS ON Artifact (from relations arrays)\n","    # We need to re-parse relations with the same schema (so we can explode it)\n","    parsed_ws_rel = (spark.createDataFrame([Row(payload_json=payload_json)])\n","        .select(from_json(col(\"payload_json\"), scan_schema).alias(\"scan\"))\n","        .select(explode(col(\"scan.workspaces\")).alias(\"w\"))\n","    )\n","\n","    def explode_depends(array_col_name: str):\n","        return (parsed_ws_rel\n","            .select(\n","                col(\"w.id\").alias(\"workspace_id\"),\n","                explode(col(f\"w.{array_col_name}\")).alias(\"a\")\n","            )\n","            .select(\n","                col(\"workspace_id\"),\n","                col(\"a.id\").alias(\"artifact_id\"),\n","                explode(col(\"a.relations\")).alias(\"rel\")\n","            )\n","            .select(\n","                lit(scan_id).alias(\"scan_id\"),\n","                lit(run_dt).alias(\"run_dt\"),\n","                col(\"workspace_id\"),\n","                col(\"artifact_id\").alias(\"from_artifact_id\"),\n","                col(\"rel.dependentOnArtifactId\").alias(\"to_artifact_id\"),\n","                col(\"rel.relationType\").alias(\"relation_type\"),\n","                col(\"rel.usage\").alias(\"usage\"),\n","            )\n","            .where(col(\"from_artifact_id\").isNotNull() & col(\"to_artifact_id\").isNotNull())\n","        )\n","\n","    depends_parts = [\n","        explode_depends(\"DataAgent\"),\n","        explode_depends(\"Reflex\"),\n","        # Add others if they have relations in your tenant later\n","    ]\n","    edge_depends_df = depends_parts[0]\n","    for d in depends_parts[1:]:\n","        edge_depends_df = edge_depends_df.unionByName(d, allowMissingColumns=True)\n","\n","    # 10) Edge: Workspace ON Capacity\n","    edge_ws_on_capacity_df = (ws_df\n","        .where(col(\"capacity_id\").isNotNull())\n","        .select(\n","            lit(scan_id).alias(\"scan_id\"),\n","            lit(run_dt).alias(\"run_dt\"),\n","            col(\"workspace_id\").alias(\"from_workspace_id\"),\n","            col(\"capacity_id\").alias(\"to_capacity_id\"),\n","        )\n","    )\n","\n","    # Write tables (graph-friendly; overwrite for “start over”)\n","    ws_df.write.format(\"delta\").mode(mode).saveAsTable(node_workspaces)\n","    artifacts_df.write.format(\"delta\").mode(mode).saveAsTable(node_artifacts)\n","    datasets_df.write.format(\"delta\").mode(mode).saveAsTable(node_datasets)\n","    cap_df.write.format(\"delta\").mode(mode).saveAsTable(node_capacities)\n","\n","    edge_ws_contains_df.write.format(\"delta\").mode(mode).saveAsTable(edge_ws_contains)\n","    edge_report_uses_dataset_df.write.format(\"delta\").mode(mode).saveAsTable(edge_report_uses_dataset)\n","    edge_depends_df.write.format(\"delta\").mode(mode).saveAsTable(edge_depends_on)\n","    edge_ws_on_capacity_df.write.format(\"delta\").mode(mode).saveAsTable(edge_ws_on_capacity)\n","\n","    return {\n","        \"raw_table\": raw_table,\n","        \"nodes\": [node_workspaces, node_artifacts, node_datasets, node_capacities],\n","        \"edges\": [edge_ws_contains, edge_report_uses_dataset, edge_depends_on, edge_ws_on_capacity],\n","    }"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"4f808cfa-6cec-4877-96fb-624ee00d8d43","normalized_state":"finished","queued_time":"2026-02-22T17:58:11.3802078Z","session_start_time":null,"execution_start_time":"2026-02-22T17:58:22.8438646Z","execution_finish_time":"2026-02-22T17:58:23.0949353Z","parent_msg_id":"ff321f60-51af-4012-a99a-ba129a7073fd"},"text/plain":"StatementMeta(, 4f808cfa-6cec-4877-96fb-624ee00d8d43, 8, Finished, Available, Finished)"},"metadata":{}}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4c44d3f6-7724-48b5-88d7-1b0bf3a54ca1"},{"cell_type":"code","source":["\n","\n","def save_scan_result_as_delta_table(scan_id: str, scan_result: dict, table_name: str = \"pbi_scan_results\"):\n","    # 1) Persist raw JSON too (optional but handy for debugging/audit)\n","    from pyspark.sql import Row\n","    from pyspark.sql.types import StructType, StructField, StringType\n","    import json\n","    from datetime import datetime\n","\n","    raw_path = f\"Files/pbi_scans/raw/scanResult_{scan_id}.json\"\n","    mssparkutils.fs.mkdirs(\"Files/delta_tables/raw\")\n","    mssparkutils.fs.put(raw_path, json.dumps(scan_result, ensure_ascii=False), overwrite=True)  # writes string to file [1](https://learn.microsoft.com/en-us/fabric/data-engineering/microsoft-spark-utilities)[2](https://learn.microsoft.com/en-us/fabric/data-engineering/notebook-utilities)\n","\n","    # 2) Create a minimal “raw JSON” delta table (schema-stable)\n","    # Store scan_id + full JSON payload as a string column\n","    payload_str = json.dumps(scan_result, ensure_ascii=False)\n","\n","    now = datetime.now()\n","\n","    df = spark.createDataFrame(\n","        [Row(scan_id=scan_id, payload_json=payload_str)],\n","        schema=StructType([\n","            StructField(\"scan_id\", StringType(), False),\n","            StructField(\"payload_json\", StringType(), False),\n","        ])\n","    )\n","\n","    # Managed delta table under Tables/\n","    (df.write\n","       .format(\"delta\")\n","       .mode(\"append\")\n","       .saveAsTable(table_name))\n","\n","    return raw_path, table_name\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"4f808cfa-6cec-4877-96fb-624ee00d8d43","normalized_state":"finished","queued_time":"2026-02-22T17:58:11.5000783Z","session_start_time":null,"execution_start_time":"2026-02-22T17:58:23.0972335Z","execution_finish_time":"2026-02-22T17:58:23.3325011Z","parent_msg_id":"121eca4c-a7c3-4fbb-8975-e5c5d923bb28"},"text/plain":"StatementMeta(, 4f808cfa-6cec-4877-96fb-624ee00d8d43, 9, Finished, Available, Finished)"},"metadata":{}}],"execution_count":7,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5b628689-ea43-4653-aa80-f0a25a89186b"},{"cell_type":"code","source":["\n","\n","def save_scan_result(scan_id: str, scan_result: dict, folder: str = \"Files/pbi_scans\") -> tuple[str, bool]:\n","    \"\"\"\n","    Save a Power BI Scanner API scan_result JSON into the Lakehouse Files area.\n","\n","    Returns:\n","      (raw_path, ok)\n","        raw_path: the full path written to\n","        ok: True if write succeeded, False otherwise (per mssparkutils.fs.put)\n","    \"\"\"\n","    import json\n","    raw_path = f\"{folder}/scanResult_{scan_id}.json\"\n","\n","    # Ensure folder exists (creates parents as needed)\n","    mssparkutils.fs.mkdirs(folder)  \n","\n","    # Write JSON file (put returns bool; overwrite supported)\n","    ok = mssparkutils.fs.put(\n","        raw_path,\n","        json.dumps(scan_result, indent=2),\n","        overwrite=True\n","    )  \n","\n","    return raw_path, ok"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":10,"statement_ids":[10],"state":"finished","livy_statement_state":"available","session_id":"4f808cfa-6cec-4877-96fb-624ee00d8d43","normalized_state":"finished","queued_time":"2026-02-22T17:58:11.5955412Z","session_start_time":null,"execution_start_time":"2026-02-22T17:58:23.3346232Z","execution_finish_time":"2026-02-22T17:58:23.5528274Z","parent_msg_id":"bf74cc1c-ce06-44bf-8648-0786080b7c7b"},"text/plain":"StatementMeta(, 4f808cfa-6cec-4877-96fb-624ee00d8d43, 10, Finished, Available, Finished)"},"metadata":{}}],"execution_count":8,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"db367290-3128-4fa3-a456-99bf7b97ad44"},{"cell_type":"code","source":["def get_access_token():\n","    import msal\n","    app = msal.ConfidentialClientApplication(\n","        client_id=CLIENT_ID,\n","        authority=f\"https://login.microsoftonline.com/{TENANT_ID}\",\n","        client_credential=CLIENT_SECRET\n","    )\n","    result = app.acquire_token_for_client(scopes=SCOPE)\n","    if \"access_token\" not in result:\n","        raise RuntimeError(f\"Token acquisition failed: {result}\")\n","    return result[\"access_token\"]\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"4f808cfa-6cec-4877-96fb-624ee00d8d43","normalized_state":"finished","queued_time":"2026-02-22T17:58:11.6923875Z","session_start_time":null,"execution_start_time":"2026-02-22T17:58:23.5550822Z","execution_finish_time":"2026-02-22T17:58:23.7837368Z","parent_msg_id":"dbe2c7ad-3974-44ab-855d-2ee8f137ffd2"},"text/plain":"StatementMeta(, 4f808cfa-6cec-4877-96fb-624ee00d8d43, 11, Finished, Available, Finished)"},"metadata":{}}],"execution_count":9,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bd7402d8-e196-4b79-9941-a092614ca7e1"},{"cell_type":"code","source":["import json\n","import time\n","import uuid\n","import requests\n","from datetime import datetime, timezone, timedelta\n","from datetime import datetime, timezone\n","from pyspark.sql import Row\n","from pyspark.sql.functions import col, explode, lit, from_json\n","from pyspark.sql.types import (\n","    StructType, StructField, StringType, BooleanType, ArrayType, TimestampType\n",")\n","\n","\n","# ---- CONFIG (fill these in from Fabric secrets / Key Vault) ----\n","# need to fix this to pull from keyvault\n","\n","TENANT_ID     = \"<Your Tenant ID>\"\n","CLIENT_ID     = \"<Your Client ID>\"\n","CLIENT_SECRET = \"<Your Secret>)\"\n","\n","\n","#\n","#  Initialize Variables used for API calls\n","#\n","SCOPE = [\"https://analysis.windows.net/powerbi/api/.default\"]\n","BASE = \"https://api.powerbi.com/v1.0/myorg\"\n","\n","token = get_access_token()\n","\n","headers = {\n","    \"Authorization\": f\"Bearer {token}\",\n","    \"Content-Type\": \"application/json\"\n","}\n","\n","#\n","#  Initialize variables used in this cell\n","#\n","CST = timezone(timedelta(hours=-6), name=\"CST\")            # Define CST as UTC -6\n","now = datetime.now(CST)                                    # Get datetime to timestamp scan runs in data\n","BATCH_SIZE = 100                                           # Number of workspaces per batch\n","batch_ids = []                                             # Used to store Batch IDs\n","scan_requests = []                                         # Used to store Scan Request IDs\n","\n","#\n","#  Loop to break workspace scans into groups of 100 and one final one of less than 100\n","#\n","\n","workspaces = list_fabric_workspaces(token)\n","\n","for ws in workspaces:  \n","\n","    name = ws.get(\"displayName\") or ws.get(\"name\") or \"<no name>\"\n","    wid  = ws.get(\"id\")\n","\n","    #print(f\"{name} | {wid}\")\n","\n","    if not wid:\n","        continue\n","\n","    batch_ids.append(wid)\n","\n","    # When we hit 100, submit a scan and reset the batch\n","    if len(batch_ids) == BATCH_SIZE:\n","        scan_req = start_scan(\n","            workspace_ids=batch_ids,\n","            lineage=True,\n","            datasourceDetails=True,\n","            datasetSchema=False,\n","            datasetExpressions=False,\n","            getArtifactUsers=False\n","        )\n","        scan_requests.append(scan_req)\n","        batch_ids = []\n","\n","# After loop ends, submit the final partial batch (1..99)\n","if len(batch_ids) > 0:\n","    scan_req = start_scan(\n","        workspace_ids=batch_ids,\n","        lineage=True,\n","        datasourceDetails=True,\n","        datasetSchema=False,\n","        datasetExpressions=False,\n","        getArtifactUsers=False\n","    )\n","    scan_requests.append(scan_req)\n","#print(\"scan request ID: \", scan_req['id'])\n","\n","\n","# \n","#  Get scan status.  I do not pull results until the scan is complete and has a status of Succeeded.\n","#\n","while True:\n","    st = get_scan_status(scan_req['id'])\n","    status = st.get(\"status\")\n","    print(\"scanStatus =\", status)\n","    if status in (\"Succeeded\", \"Failed\"):\n","        break\n","    time.sleep(3)\n","\n","if status != \"Succeeded\":\n","    raise RuntimeError(f\"Scan failed: {st}\")\n","print(scan_req)\n","\n","# path, ok = save_scan_result(scan_req['id'], )\n","\n","#\n","#  Write the scan results and workspace info to OneLake as delta parquet\n","#\n","\n","# write scans to delta\n","#wresult = save_scan_result_as_delta_table_dt(\n","#    scan_id=scan_req['id'], \n","#    scan_result=get_scan_result(scan_req['id']),\n","#    run_dt=now,\n","#    table_name=\"scans\" )\n","\n","#write workspaces to delta\n","#wsresult = save_scan_result_as_delta_table_dt(\n","#    scan_id=scan_req['id'], \n","#    scan_result=workspaces,\n","#    run_dt=now, \n","#    table_name=\"workspaces\")\n","\n","out = persist_scan_to_graph_tables(\n","    scan_id=scan_req['id'],\n","    scan_result=get_scan_result(scan_req['id']),\n","    run_dt=now,\n","    mode=\"append\")\n","\n","print(\"run date time: \",now)\n","print(out)\n","print(f\"Submitted {len(scan_requests)} scans\")\n","print(wid)\n","print(batch_ids)\n","\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":13,"statement_ids":[13],"state":"finished","livy_statement_state":"available","session_id":"4f808cfa-6cec-4877-96fb-624ee00d8d43","normalized_state":"finished","queued_time":"2026-02-22T18:01:02.0864075Z","session_start_time":null,"execution_start_time":"2026-02-22T18:01:02.0876186Z","execution_finish_time":"2026-02-22T18:01:02.326214Z","parent_msg_id":"89252879-19eb-438a-8935-a53062b2d245"},"text/plain":"StatementMeta(, 4f808cfa-6cec-4877-96fb-624ee00d8d43, 13, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["{'raw_table': 'pbi_scan_results_raw', 'nodes': ['pbi_graph_workspaces', 'pbi_graph_artifacts', 'pbi_graph_datasets', 'pbi_graph_capacities'], 'edges': ['pbi_graph_edge_workspace_contains_artifact', 'pbi_graph_edge_report_uses_dataset', 'pbi_graph_edge_artifact_depends_on_artifact', 'pbi_graph_edge_workspace_on_capacity']}\n"]}],"execution_count":11,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6e715c6b-c5ce-4f08-a07c-2958588e8335"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"2b90bdeb-0eae-4e55-af39-2a06759fae1a","known_lakehouses":[{"id":"2b90bdeb-0eae-4e55-af39-2a06759fae1a"}],"default_lakehouse_name":"lineageGraph","default_lakehouse_workspace_id":"bdbf1597-b6ea-43bf-8888-6b735806eb23"}}},"nbformat":4,"nbformat_minor":5}